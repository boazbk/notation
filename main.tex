\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{fullpage}
\usepackage{xcolor}
\usepackage{namedtensor}
% "experimental" notation
\newcommand{\nndot}[2]{\mathbin{\mathop{\boldsymbol\cdot}\limits_{\name{#1}|\name{#2}}}}

% for formal definitions
\DeclareMathOperator{\ind}{ind}
\DeclareMathOperator{\rec}{rec}
\newcommand{\restrict}[2]{\left.#1\right|_{#2}}
\newcommand{\nmatrix}[3]{\name{#1}\begin{array}[b]{@{}c@{}}\name{#2}\\\begin{bmatrix}#3\end{bmatrix}\end{array}}

\DeclareMathOperator*{\softmax}{softmax}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\dff}{d_{\text{ff}}}

\usepackage{parskip}
\setcounter{tocdepth}{2}
\usepackage{natbib}
\usepackage[hidelinks]{hyperref}

\title{Named Tensor Notation}
\author{David Chiang, Sasha Rush, and Boaz Barak}

\newcommand{\colA}[1]{\textcolor{blue}{#1}}
\newcommand{\colB}[1]{\textcolor{red}{#1}}
\newcommand{\colC}[1]{\textcolor{orange}{#1}}
\newcommand{\colD}[1]{\textcolor{purple}{#1}}


\newaxis{channel}
\newaxis{width}
\newaxis{height}
\newaxis{batch}
\newaxis{seq}
\newaxis{key}
\newaxis{time}
\newaxis{emb}
\newaxis{head}
\newaxis{ax} 
\newaxis{layer}
\newaxis{foo}
\newaxis{bar}
\newaxis{hidden}
\newaxis{val}

\newcommand{\qt}[1]{\text{`#1'}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}



\newcommand{\stimes}{\!\vcenter{\hbox{\text{\tiny $\times$}}}\!}

\begin{document}

\maketitle

%\tableofcontents

%\clearpage

\begin{abstract}
We propose a notation for tensors with named axes. For example, instead of writing $A \in \R^{b\times w \times h \times c}$ for an order-4 tensor corresponding to a batch of images, we use $A \in \R^{\batch \times \width \times \height \times \channel}$.  Such a tensor can be indexed using these names, as in $A_{\batch(25),\width(12),\height(30),\channel(1)}$, which refers to the same index as $A_{\channel(1),\width(12),\height(30),\batch(25)}$.
Named notation relieves the author, reader, and future implementers from needing to keep track of the order of parameters. It also makes it easier to express ``slices'' of tensors, such as $A_{\batch(17)}$ or $A_{\channel(\qt{Red})}$, and to seamlessly extend operations on low-order tensors to higher order ones (e.g., extend an operation on images to batches of images, or extend the attention mechanism to multiple tokens and heads).

In this document we present the notation, and illustrate it through several examples from modern machine learning, including both building blocks such as Attention and Convolutions, as well as full network architectures such as Transformer and LeNet. Most of our proposals are not original, and we borrow ideas from several works and packages. We hope that by putting together this document, we can encourage more authors to use named tensors, resulting in clearer papers and less bug-prone implementations.

The source code for this document can be found at \url{https://github.com/namedtensor/notation/}. We invite anyone to make comments on this proposal by submitting issues or pull requests on this repository.
\end{abstract} 

\section{Introduction}
\label{sec:intro}

Most papers about neural networks use the notation of vectors and matrices from applied linear algebra. This notation is very well-suited to talking about vector spaces, but becomes cumbersome and confusing when extended to the high-order tensors of diverse axes used in modern neural networks. Consider the following equation for the \emph{Attention} mechanism  \citep{vaswani+:2017}:

\begin{equation}
  \text{Att}(Q\;,\; K\;,\; V) = \softmax \left( \frac{QK^\top}{\sqrt{d_k}} \right) V \;.  \label{eq:attentionbefore}
\end{equation}
where $Q$, $K$, and $V$ are sequences of query, key, and value vectors packed into matrices. Does the product $QK^\top$ sum over the sequence, or over the query/key features? We would need to know the sizes of $Q$, $K$, and $V$ to know that it's taken over the query/key features. Is the softmax taken over the query sequence or the key sequence? The usual notation doesn't even offer a way to answer this question. 
With multiple attention heads and multiple sentences in a minibatch, $Q$ and $K$ become tensors with four separate axes/dimensions, and most papers skip a full description with all these details. 

% Back in the realm of mathematical notation, then, we want two things: first, the flexibility of working with multidimensional arrays, and second, the perspicuity of identifying axes by name instead of by position. This document describes our proposal to do both.


In this document  we propose an intuitive mathematical notation for tensors with \emph{named axes}. The notation has a formal underpinning, but it is intuitive enough that researchers in machine learning should be able to follow it without needing to look any concepts up. 
As a preview, in our notation, Equation (\ref{eq:attentionbefore}) becomes

\begin{flalign}
  &\text{Att}\!:\!\R^\key \stimes \R^{\seq \times \key} \stimes \R^{\seq} \rightarrow \R \nonumber &\\ 
  &\text{Att}(\;Q\;,\;K\;\;,\;\;V) \;\;\;\;= \nfun{seq}{softmax} \left( \frac{Q \ndot{key} K}{\sqrt{|\name{key}|}} \right) \ndot{seq} V \;. & \label{eq:attentionafter} 
\end{flalign}

Unlike (\ref{eq:attentionbefore}), in Equation~(\ref{eq:attentionafter}) the reader does not need to remember whether it is the columns or the rows of $K$ that correspond to distinct queries, since the notation makes it clear that the dot product inside the soft max is taken along the \key\ axis, resulting in a vector in $\R^\seq$. The softmax operation is then applied to every element of this vector (using the whole vector for the normalization), and we take a dot product of it with $V$ along the $\seq$ axis.
The formula generalizes \emph{as written} to the case that each element is $V_{\seq(i)}$ is itself a vector, we have multiple heads, tokens. a minibatch.
This will correspond to adding a $\seq$ axis to $Q$ (to allow for a sequence of  input tokens), adding a $\val$ axis to $V$,  and adding the $\head$ and $\batch$ axes to all input tensors.
Our convention is that these additional input axes ``pass through'' to the output.
Thus instead of a single number, the output will be a tensor in $\R^{\seq \times \val \times \head \times \batch}$, each coordinate of which corresponds to invoking the original operation on the corresponding ``slice'' of the input tensors. The names ensure that we can properly align these slices. 
This is illustrated in Equation~(\ref{eq:attentionexpand}) below, where we use color to highlight the axes that are not captured by the operator $\text{Att}$'s signature, and use identical colors for axes that will be aligned in the output.\footnote{In this example, the axis $\seq$ of $K$ and $V$ is captured by the signature, and hence is not aligned with the axis $\colA{\seq}$ of $Q$ and need not even be of the same length. We can also emphasize this by using a different name such as $\seq'$ or $\seq^{in}$ for $R$'s axis, and use our renaming/casting operator $Y_{\seq' \rightarrow \seq}$ on the output.}

\begin{flalign}
  &Q \in \R^{\key\times \colA{\seq} \times \colB{\head} \times \colC{\batch}},  K \in \R^{\key\times \seq \times \colB{\head} \times \colC{\batch}},
  V \in \R^{\seq \times \colD{\val} \times \colB{\head} \times \colC{\batch}} \\
  &\text{Att}\!:\!\R^\key \stimes \R^{\seq \times \key} \stimes \R^{\seq} \rightarrow \R \nonumber &\\ 
  &\text{Att}(\;Q\;,\;K\;\;,\;\;V) \;\;\;\;= \nfun{seq}{softmax} \left( \frac{Q \ndot{key} K}{\sqrt{|\name{key}|}} \right) \ndot{seq} V  \in \R^{\colA{\seq} \times \colB{\head} \times \colC{\batch} \times \colD{\val}}\;. & \label{eq:attentionexpand} 
\end{flalign}


Our notation is inspired by several libraries that have been developed to identify axes by \emph{name} instead: Nexus \citep{chen2017typesafe}, tsalib \citep{tsalib}, NamedTensor \citep{namedtensor}, named tensors in PyTorch \citep{named-tensors}, and Dex \citep{maclaurin+:2019}.
However, in this work our focus is on mathematical notation.
We hope that widespread adoption of named tensors in papers will lead to their adoption in code as well.
We welcome comments on this document, as well as additional examples, via issues or pull requests on the repository \url{https://github.com/namedtensor/notation/}. 


\section{Summary of notation}

We now provide a very quick overview of the proposed notation, leaving detailed examples to \S\ref{sec:examples} and formal  mathematical definitions to \S\ref{sec:definitions}.
A standard (``un-named'') vector, matrix, or tensors is simply a map from one or more intervals of integers to the real numbers.
For example, a length $n$ vector is a map $v\in \R^{[n]}$, where $[n]= \{1,2,\ldots,n \}$, an $n\times m$ matrix is a map $A \in \R^{[n]\times [m]}$, an $n\times m \times k$ three-tensor is a map of the form $T\in \R^{[n]\times [m] \times [k]}$ and so on and so forth. 
For a tensor $T \in \R^{[n]\times [m]\times [k]}$ and $(i,j,k) \in [n]\times [m]\times [k]$, we typically denote by $T_{i,j,k}$ the real number that $(i,j,k)$ is mapped to by $T$.

\emph{Named tensors} are maps from \emph{axes} to the real numbers. For example, an image might be represented as a named tensor $I \in \R^{\width \times \height \times \channel}$. You can think of the axes as intervals of integers,\footnote{Though we will sometimes include axes ranging over sets such as $\{ \qt{Red},\qt{Green},\qt{Blue} \}$ for RGB channels or words, or an axis corresponding to all words in some dictionary or collection.} but one endowed with a \emph{type}, in the sense that (for example) the $7^{th}$ element of the $\width$ axis, which we denote by $\width(7)$, is considered distinct from the $7^{th}$ element of the $\height$ axis.
In cases where the size of an ax is not fixed  or can be inferred from context, we use names such as  $\width,\height,\channel$.
Otherwise, we use names such as $\width[w],\height[h],\channel[c]$ (with $w,h,c \in \N$) to specify particular sizes, and so write $I \in \R^{\width[w]\times \height[h]\times\channel[c]}$. If $x,y,a$ is a point in  $(\width,\height,\channel)$ coordinates, then we use $I_{\width[x],\height[y],\channel[c]}$ to denote the corresponding value of $I$. This notation is unordered, and so  $I_{\channel[c],\height[y],\width[x]}$ denotes the same value.
Similarly, $\R^{\width \times \height \times \channel}$ is the same set as $\R^{\channel \times \height \times \width}$.
We also use partial indexing to denote \emph{slices}, and so, for example, $I_{\width[22]}$ denotes the $x=22$ slice of the image $I$.

\paragraph{Operations on tensors.}  An \emph{operator} on tensors is a function $F$ mapping one or more tensors into a tensor. The \emph{signature} of $F$ corresponds to the types (names of axes) of each of the input tensors and of the output tensors. If some of the input tensors have additional axes that are not contained in the signature, then these would be added to the output tensors in the following way. If any of those additional axes are in only one of the input tensors, then they are considered \emph{dangling axes} and passed as they are to the output tensors. For example, if $F:\R^{\width\times\height\times\channel} \rightarrow \R^{\layer}$ is an operator, and we pass to $F$ a tensor $X \in \R^{\width\times\height\times\channel\times\batch}$, then $\batch$ is a dangling axis, and $F(X)$ is the tensor $Y$ in $R^{\layer \times \batch}$ such that for every $\batch$ index $i$ of $X$ (for which we sometimes use the notation $i\in \batch(X)$), $Y_{\batch(i)} = F(X_{\batch(i)})$.
If two or more of the input tensor share the same axis (i.e., with the same name), then these are considered \emph{aligned} axes.
This axis $\ax$ should have the same support in all the input tensors, and the output will contain $\ax$ with the slice $\ax(i)$ of the output corresponding to the invocation of $F$ on the slices $\ax(i)$ of all inputs with the axis $\ax$.
For example, if $F$ is the operation that takes two tensors $A,B \in \R^{\width \times \height}$ and produces some number $F(A,B)$ corresponding to their similarity, then if $X,W \in \R^{\width\times \height \times \channel \times \batch}$ having the same support over the $\channel$ and $\batch$ axes, then $F(X,W)$ will be a tensor $Y \in \R^{\channel \times \batch}$ such that for every $c$ in the support of $\channel$ and $b$ in the support of $\batch$, $F(X,W)_{\channel(c),\batch(b)} = F(X_{\channel(c),\batch(b)},W_{\channel(c),\batch(b)})$. 

\paragraph{Contractions.} Named tensors allow us to explicitly state which axis \emph{contractions} such as sum, dot product, mean, variance, etc. operate on.
For example, if $X \in \R^{\width\times \height \times \channel \times \batch}$ then $\asum{\batch} X = \sum_{b \in \batch(X)}X_{\batch(b)}$ is the tensor in $\R^{\width \times \height \times \channel}$ which is sum of all images across all batches, while $\asum{\channel} X = \sum_{c \in \channel(C)}X_{\channel(c)}$  is the tensor in $\R^{\width \times \height \times \batch}$ which denotes the sum of the channels for every pixel and batch index.
We also use similar notation for the dot product. For example, if $X \in \R^{\layer \times \batch}$ and $W \in \R^{\layer \times \hidden}$ then $X \adot{\layer} W$ is the tensor $Y$ in $\R^{\hidden \times \batch}$ such that for every $b \in \batch(X)$ and $h \in \hidden(W)$, $Y_{\batch(b),\hidden(h)} = \sum_{\ell \in \layer} X_{\layer(\ell)} W_{\layer(\ell)}$.
In other words, $X \adot{\layer} W$ equals to the matrix product $WX$ where we think of $W$ as a $\hidden \times \layer$ matrix and of $X$ as a $\layer \times \batch$ matrix.

\iffalse

\section{Informal Overview}
\label{sec:intro}

Let's think first about the usual notions of vectors, matrices, and tensors, without named axes.

Define $[n] = \{1, \ldots, n\}$. We can think of a size-$n$ real vector $v$ as a function from $[n]$ to $\mathbb{R}$. We get the $i$th element of $v$ by applying $v$ to $i$, but we normally write this as $v_i$ (instead of $v(i)$). 

Similarly, we can think of an $m \times n$ real matrix as a function from $[m] \times [n]$ to $\mathbb{R}$, and an $l \times m \times n$ real tensor as a function from $[l] \times [m] \times [n]$ to $\mathbb{R}$. In general, then, real tensors are functions from \emph{tuples of natural numbers} to reals.

\subsection{Named tensors}

We want to make tensors into functions, no longer on tuples, but on \emph{records}, which look like this: \[\{\nidx{foo}{1}, \nidx{bar}{3}\}\] where $\name{foo}$ and $\name{bar}$ are \emph{names} (written in sans-serif font), mapped to 1 and 3, respectively. The pairs $\nidx{foo}{1}$ and $\nidx{bar}{3}$ are called \emph{named indices}. Their order doesn't matter: $\{\nidx{foo}{1}, \nidx{bar}{3}\}$ and $\{\nidx{bar}{3}, \nidx{foo}{1}\}$ are the same record.

The set of records that can be used to index a named tensor is defined by a \emph{shape}, which looks like this: \[\nset{foo}{2} \times \nset{bar}{3}\] which stands for records where $\name{foo}$'s index ranges from 1 to 2, and $\name{bar}$'s index ranges from 1 to 3. The pairs $\nset{foo}{2}$ and $\nset{bar}{3}$ are called \emph{axes}, and again their order doesn't matter: $\nset{foo}{2} \times \nset{bar}{3}$ and $\nset{bar}{3} \times \nset{foo}{2}$ are the same shape.
\fi

\section{Overview of our notation}


For every string ``$\text{ax}$'', an \emph{axis} with the name ``$\text{ax}$'', which we denote by $\ax$ in san-serif font, is a set of pairs  $\ax = \{ (\text{ax}, i) \;|\; i \in X \}$ where $X$ is some ordered set (most typically, and by default, $X$ is an interval of integers starting with $1$). The set $X$ is known as the \emph{index set} of $\ax$. We use the notation $\ax(i)$ for the pair $(\text{ax},i)$ and $\ax[n]$ to denote the set $\{ (\text{ax},i) | i \in [n] \}$. More generally for every subset $S$ of $\ax$'s index set, we denote the set $\{ (\text{ax},i) | i \in S \}$ by $\ax[S]$.

If $\foo$ and $\bar$ are axes, a (real-valued) \emph{named tensor} over the axes $\foo$, $\bar$ is a function $A$ that maps pairs $(i,j) \in \foo[S]\times \foo[T]$ into a real number, where $S,T$ are some subsets of $\foo$ and $\bar$'s index sets respectively. 
We write this as $A \in \R^{\foo \times \bar}$. We use the notation $A \in \R^{\foo[f],\bar[b]}$ when we want to explicitly state the lengths of each of $A$'s axes, and indicate that $A$ is defined over all  $(i,j) \in \foo[f] \times \bar[b]$.
For example, here is a tensor with shape $\nset{foo}{2} \times \nset{bar}{3}$.
\begin{equation*}
A = \nmatrix{foo}{bar}{
  3 & 1 & 4 \\
  1 & 5 & 9
}.
\end{equation*}

We access elements of $A$ using subscripts: $A_{\nidx{foo}{1}, \nidx{bar}{3}} = 4$.
We also allow partial indexing:
\begin{align*}
A_{\nidx{foo}{1}} &= \nmatrix{}{bar}{
  3 & 1 & 4
}
&
A_{\nidx{bar}{3}} &= \nmatrix{}{foo}{
  4 & 9
}.
\end{align*}

We use uppercase italic letters for variables standing for named tensors. We don't mind if you use another convention, but urge you not to use different styles for tensors and their elements. For example, if $\mathbf{A}$ is a tensor, then an element of $\mathbf{A}$ is written as $\mathbf{A}_{\nidx{foo}{2}, \nidx{bar}{3}}$ -- 
not $A_{\nidx{foo}{2},\nidx{bar}{3}}$ or $a_{\nidx{foo}{2},\nidx{bar}{3}}$.

Just as the set of all size-$n$ real vectors is written $\mathbb{R}^n$, and the set of all $m\times n$ real matrices is often written $\mathbb{R}^{m \times n}$, we write $\mathbb{R}^{\nset{foo}{2} \times \nset{bar}{3}}$ for the set of all tensors with shape $\nset{foo}{2} \times \nset{bar}{3}$.

In many contexts, an axis name is used with only one size. In this case, you can simply write $\name{foo}$ for the unique axis with name $\name{foo}$, as in $\mathbb{R}^{\name{foo} \times \name{bar}}$. It's common to leave the size of an axis unspecified at first, and specify its size later (like in a section or appendix on experimental details). To do this, write $|\name{foo}|=2$, $|\name{bar}|=3$.

What are good choices for axis names? We recommend meaningful \emph{words} instead of single letters, and we recommend words that describe a \emph{whole} rather than its parts. For example, a minibatch of sentences, each of which is a sequence of one-hot vectors, would be represented by a tensor with three axes, which we might name $\name{batch}$, $\name{seq}$, and $\name{vocab}$. Please see \S\ref{sec:examples} for more examples.

\subsection{Named tensor operations}
\label{sec:operations}

\subsubsection{Elementwise operations}
\label{sec:elementwise}

Any function from scalars to scalars can be applied elementwise to a named tensor:
\begin{equation*}
\exp A = \nmatrix{foo}{bar}{
  \exp 3 & \exp 1 & \exp 4 \\
  \exp 1 & \exp 5 & \exp 9
}.
\end{equation*}
More elementwise unary operations:
\[\begin{array}{cl}
kA & \text{scalar multiplication by $k$} \\
-A & \text{negation} \\
A^k & \text{elementwise exponentiation} \\
\sqrt{A} &\text{elementwise square root} \\
\exp A & \text{elementwise exponential function} \\
\tanh A & \text{hyperbolic tangent} \\
\sigma(A) & \text{logistic sigmoid} \\
\text{relu}(A) & \text{rectified linear unit}
\end{array}\]

Any function or operator that takes two scalar arguments can be applied elementwise to two named tensors with the same shape. If $A$ is as above and
\begin{equation*}
B = \nmatrix{foo}{bar}{
  2 & 7 & 1 \\
  8 & 2 & 8
}
\end{equation*}
then
\begin{equation*}
A + B = \nmatrix{foo}{bar}{
  3+2 & 1+7 & 4+1 \\
  1+8 & 5+2 & 9+8
}.
\end{equation*}

But things get more complicated when $A$ and $B$ don't have the same shape. If $A$ and $B$ each have an axis with the same name (and size), the two axes are \emph{aligned}, as above. But if $A$ has an axis named $\name{foo}$ and $B$ doesn't, then we do \emph{broadcasting}, which means effectively that we replace $B$ with a new tensor $B'$ that contains a copy of $B$ for every value of axis $\name{foo}$.
\begin{align*}
A + 1 &= \nmatrix{foo}{bar}{
  3+1 & 1+1 & 4+1 \\
  1+1 & 5+1 & 9+1
} \\
A + B_{\nidx{foo}{1}} &= \nmatrix{foo}{bar}{
  3+2 & 1+7 & 4+1 \\
  1+2 & 5+7 & 9+1
} \\
A + B_{\nidx{bar}{3}} &= \nmatrix{foo}{bar}{
  3+1 & 1+1 & 4+1 \\
  1+8 & 5+8 & 9+8
}.
\end{align*}
Similarly, if $B$ has an axis named $\name{foo}$ and $A$ doesn't, then we effectively replace $A$ with a new tensor $A'$ that contains a copy of $A$ for every value of axis $\name{foo}$. If you've programmed with NumPy or any of its derivatives, this should be unsurprising to you.

More elementwise binary operations:
\[\begin{array}{cl}
A+B & \text{addition} \\
A-B & \text{subtraction} \\
A\odot B & \text{elementwise (Hadamard) product} \\
\displaystyle\frac{A}{B} & \text{elementwise division} \\[1.2ex]
\max \{A, B\} & \text{elementwise maximum} \\
\min \{A, B\} & \text{elementwise minimum}
\end{array}\]

\subsubsection{Reductions}

The same rules for alignment and broadcasting apply to functions that take tensor as arguments or return tensors. The gory details are in \S\ref{sec:tensorfunctions}, but we present the most important subcases here. The first is \emph{reductions}, which are functions from vectors to scalars. Unlike with functions on scalars, we always have to specify which axis these functions apply to, using a subscript. (This is equivalent to the \verb|axis| argument in NumPy and \verb|dim| in PyTorch.)

For example, using the same example tensor $A$ from above,
\begin{align*}
\nsum{foo} A &= \nmatrix{}{bar}{
  3+1 & 1+5 & 4+9
} \\
\nsum{bar} A &= \nmatrix{}{foo}{
  3+1+4 & 1+5+9
}.
\end{align*}
More reductions: If $A$ has an axis $\nset{foo}{I}$, then
\begin{align*}
  \nsum{foo} A &= \sum_{i \in I} A_{\nidx{foo}{i}} = \nmatrix{}{bar}{4 & 6 & 13} \\
  \nfun{foo}{norm} A &= \sqrt{\nsum{foo} A^2} = \nmatrix{}{bar}{\sqrt{10} & \sqrt{26} & \sqrt{97}} \\
  \nfun{foo}{min} A &= \min_{i \in I} A_{\nidx{foo}{i}} = \nmatrix{}{bar}{1 & 1 & 4} \\
  \nfun{foo}{max} A &= \max_{i \in I} A_{\nidx{foo}{i}} = \nmatrix{}{bar}{3 & 5 & 9} \\
  \nfun{foo}{mean} A &= \frac{1}{|I|} A = \nmatrix{}{bar}{2 & 3 & 6.5} \\
  \nfun{foo}{var} A &= \frac1{|I|} \nsum{ax} (A - \nfun{foo}{mean} A)^2 = \nmatrix{}{bar}{1 & 4 & 6.25}
\end{align*}
(Note that $\max$ and $\min$ are overloaded; with multiple arguments and no subscript, they are elementwise, and with a single argument and a subscript, they are reductions.)

You can also write multiple names to perform the reduction over multiple axes at once.

\subsubsection{Contraction}

The vector dot product (inner product) is a function from \emph{two} vectors to a scalar, which generalizes to named tensors to give the ubiquitous \emph{contraction} operator, which performs elementwise multiplication, then sums along an axis. It can be used, for example, for matrix multiplication:
\begin{align*}
C &= \nmatrix{bar}{baz}{
  1 & -1 \\ 2 & -2 \\ 3 & -3
} \\
A \ndot{bar} C &= \nmatrix{foo}{baz}{
  17 & -17 \\
  53 & -53
}
\end{align*}
However, note that (like vector dot-product, but unlike matrix multiplication) this operator is commutative, but not associative! Specifically, if
\begin{align*}
A &\in \mathbb{R}^{\nset{foo}{m}} \\
B &\in \mathbb{R}^{\nset{foo}{m} \times\nset{bar}{n}} \\
C &\in \mathbb{R}^{\nset{foo}{m} \times\nset{bar}{n}}
\end{align*}
then $(A \ndot{foo} B) \ndot{bar} C$ and $A \ndot{foo} (B \ndot{bar} C)$ don't even have the same shape.

\subsubsection{Vectors to vectors}

A very common example of a function from vectors to vectors is the softmax:
\begin{equation*}
  \nfun{foo}{softmax} A = \frac{\exp A}{\nsum{foo} \exp A} \approx \nmatrix{foo}{bar}{
    0.881 & 0.018 & 0.007 \\
    0.119 & 0.982 & 0.993
  }.
\end{equation*}
Also its ``hard'' version, which maps a vector to a one-hot vector where the maximum element becomes 1 and all other elements become 0 (breaking ties arbitrarily):
\begin{equation*}
  \nfun{foo}{argmax} A = \nmatrix{foo}{bar}{
    1 & 0 & 0 \\
    0 & 1 & 1
  }.
\end{equation*}

Concatenation combines two vectors into one:
\begin{align*}
  A \ncat{foo} B &= \nmatrix{foo}{bar}{
    3 & 1 & 4 \\
    1 & 5 & 9 \\
    2 & 7 & 1 \\
    8 & 2 & 8
  } \\
  A \ncat{bar} B &= \nmatrix{foo}{bar}{
    3 & 1 & 4 & 2 & 7 & 1 \\
    1 & 5 & 9 & 8 & 2 & 8
  }
\end{align*}

\subsubsection{Renaming and reshaping}
\label{sec:reshaping}

It's also very handy to have a function that renames an axis:
\begin{equation*}
\nmov{bar}{baz}{A} = \nmatrix{foo}{baz}{
  3 & 1 & 4 \\
  1 & 5 & 9
}
\end{equation*}

We can reshape two or more axes into one axis:
\begin{equation*}
  \nmov{(foo,bar)}{baz}{A} = \nmatrix{}{baz}{
    3 & 1 & 4 & 1 & 5 & 9
  }
\end{equation*}
Similarly, we can reshape one axis into two or more axes, or even multiple axes into multiple axes.
The order of elements in the new axis or axes is undefined. If you need a particular order, you can write a more specific definition.

\subsubsection{Matrices}

Finally, we briefly consider functions on matrices, for which you have to give \emph{two} axis names (and the order in general matters). Let $A$ be a named tensor with shape $\nset{foo}{2} \times\nset{bar}{2} \times\nset{baz}{2}$:
\begin{align*}
A_{\nidx{foo}{1}} &= \nmatrix{bar}{baz}{
  1 & 2 \\
  3 & 4
} \\
A_{\nidx{foo}{2}} &= \nmatrix{bar}{baz}{
  5 & 6 \\
  7 & 8
} \\
\nfun{bar,baz}{det} A &= \nmatrix{}{foo}{\det \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} & \det \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}} \\
\nfun{baz,bar}{det} A &= \nmatrix{}{foo}{\det \begin{bmatrix} 1 & 3 \\ 2 & 4 \end{bmatrix} & \det \begin{bmatrix} 5 & 7 \\ 6 & 8 \end{bmatrix}} \\
\nfun{foo,bar}{det} A &= \nmatrix{}{baz}{\det \begin{bmatrix} 1 & 3 \\ 5 & 7 \end{bmatrix} & \det \begin{bmatrix} 2 & 4 \\ 6 & 8 \end{bmatrix}}
\end{align*}
For matrix inverses, there's no easy way to put a subscript under $\mathord\cdot^{-1}$, so we recommend writing $\nfun{foo,bar}{inv}$.

\section{Examples}
\label{sec:examples}

In this section we give a series of examples illustrating how to use named tensors in various situations, mostly related to machine learning.

\subsection{Building blocks}

\subsubsection{Fully-connected layers}

A feedforward neural network looks like this:
\begin{align*}
  X^0 &\in \mathbb{R}^{\name{input}} \\
  X^1 &= \sigma(W^1 \ndot{input} X^0 + b^1) & W^1 &\in \mathbb{R}^{\name{hidden1} \times \name{input}} & b^1 &\in \mathbb{R}^{\name{hidden1}} \\
  X^2 &= \sigma(W^2 \ndot{hidden1} X^1 + b^2) & W^2 &\in \mathbb{R}^{\name{hidden2} \times \name{hidden1}} & b^2 &\in \mathbb{R}^{\name{hidden2}} \\
  X^3 &= \sigma(W^3 \ndot{hidden2} X^2 + b^3) & W^3 &\in \mathbb{R}^{\name{output} \times \name{hidden2}} & b^3 &\in \mathbb{R}^{\name{output}}
\end{align*}
The layer sizes can be set by writing $|\name{input}| = 100$, etc. Alternatively, we could have called the axes $\name{layer}[n_0]$, $\name{layer}[n_1]$, etc. and set the $n_l$.

If you don't like repeating the equations for fully-connected layers, you can put them inside a function:
\begin{align*}
  \text{FullConn}^l(x) &= \nmov{layer'}{layer}{\sigma\left(W^l \ndot{layer} x + b^l\right)}
\end{align*}
where
\begin{align*}
  W^l &\in \mathbb{R}^{\nset{layer}{n_{l}} \times \nset{layer}{n_{l-1}}} \\
  b^l &\in \mathbb{R}^{\nset{layer}{n_l}}.
\end{align*}
Now $\text{FullConn}^l$ encapsulates both the equation for layer $l$ as well as its parameters (analogous to what TensorFlow and PyTorch call \emph{modules}).

Then the network can be defined like this:
\begin{align*}
  X^0 &\in \mathbb{R}^{\nset{layer}{n_0}} \\
  X^1 &= \text{FullConn}^1(X^0) \\
  X^2 &= \text{FullConn}^2(X^1) \\
  X^3 &= \text{FullConn}^3(X^2).
\end{align*}

\subsubsection{Recurrent neural networks}
\label{sec:rnn}

As a second example, let's define a simple (Elman) RNN. This is similar to the feedforward network, except that the number of timesteps is variable and they all share parameters.
\begin{align*}
x^{t} &\in \mathbb{R}^{\name{input}} & t &= 1, \ldots, n \\
W^{\text{h}} &\in \mathbb{R}^{\name{hidden} \times \name{hidden'}} & |\name{hidden}| &= |\name{hidden'}| \\
W^{\text{i}} &\in \mathbb{R}^{\name{input} \times \name{hidden'}} \\
b &\in \mathbb{R}^{\name{hidden'}} \\
h^{0} &\in \mathbb{R}^{\name{hidden}} \\
h^{t} &= \nmov{hidden'}{hidden}{\sigma\left( W^{\text{h}} \ndot{hidden} h^{t-1} + W^{\text{i}} \ndot{input} x^{t} + b \right)} & t &= 1, \ldots, n
\end{align*}
Here the axis name $\name{state}$ has to stay the same across time, and the renaming is necessary because our notation doesn't provide a one-step way to apply a linear transformation ($W^{\text{h}}$) to one axis and put the result in the same axis. For possible solutions, see \S\ref{sec:duality}.

\subsubsection{Attention}
\label{sec:attention}

In the introduction (\S\ref{sec:intro}), we mentioned some difficulties in interpreting the equation for attention as it's usually written. In our notation, it looks like this:
\begin{align*}
  \text{Att} \colon \mathbb{R}^{\name{key}} \times \mathbb{R}^{\name{seq} \times\name{key}} \times \mathbb{R}^{\name{seq} \times\name{val}} &\rightarrow \mathbb{R}^{\name{val}} \\
  \text{Att}(Q,K,V) &= \nfun{seq}{softmax} \left( \frac{Q \ndot{key} K}{\sqrt{|\name{key}|}} \right) \ndot{seq} V.
\end{align*}

Sometimes we need to apply a mask to keep from attending to certain positions.
\begin{align*}
  \text{Att} \colon \mathbb{R}^{\name{key}} \times \mathbb{R}^{\name{seq} \times\name{key}} \times \mathbb{R}^{\name{seq} \times\name{val}} \times \mathbb{R}^{\name{seq}} &\rightarrow \mathbb{R}^{\name{val}} \\
\text{Att}(Q, K, V, M) &= \nfun{seq}{softmax} \left( \frac{Q \ndot{key} K}{\sqrt{|\name{key}|}} + M \right) \ndot{seq} V.
\end{align*}

Models often use attention to compute a sequence of values, not just a single value. If $Q$ has (say) a \name{seq'} axis, then the above definition computes a sequence of values along the \name{seq'} axis. If $Q$, $K$, and $V$ have a \name{head} axis for multiple attention heads, then it will compute multi-head attention.

\subsubsection{Convolution}

A 1-dimensional convolution can be easily written by unrolling a tensor and then
applying a standard dot product.
\begin{align*}
\text{conv1d} \colon \reals^{\name{channels} \times \nset{seq}{n}} &\rightarrow \mathbb{R}^{\nset{seq}{n'}} \\
\text{conv1d}(X; W, b) &= W \ndot{channels, kernel} U + b
\end{align*}
where
\begin{align*}
n' &= n-|\name{kernel}|+1 \\
W &\in \reals^{\name{channels} \times \name{kernel}} \\
U &\in \reals^{\name{channels} \times \nset{seq}{n'} \times \name{kernel}} \\
U_{\nidx{seq}{i}, \nidx{kernel}{j}} & = X_{\nidx{seq}{i+j - 1}} \\
b &\in \reals.
\end{align*} 

A 2-dimensional convolution:
\begin{align*}
  \text{conv2d} \colon \reals^{\name{channels} \times \nset{height}{h} \times \nset{width}{w}} %\times \reals^{\name{channels} \times \name{kh} \times \name{kw}} \times \reals
  &\rightarrow \reals^{\nset{height}{h'} \times \nset{width}{w'}} \\
  \text{conv2d}(X; W, b) &= W \ndot{channels, kh, kw} U + b
\end{align*}  
where
\begin{align*}
h' &= h-|\name{kh}|+1 \\
w' &= w-|\name{kw}|+1 \\
W &\in \reals^{\name{channels} \times \name{kh} \times \name{kw}} \\
U &\in \reals^{\name{channels} \times \nset{height}{h'} \times \nset{width}{w'} \times \name{kh} \times \name{kw}}  \\
U_{\nidx{height}{i}, \nidx{width}{j}, \nidx{kh}{ki}, \nidx{kw}{kj}} &= X_{\nidx{height}{i+ki-1}, \nidx{width}{j+kj-1}} \\
b &\in \reals.
\end{align*}

\subsubsection{Max pooling}

\begin{align*}
\text{maxpool2d}_{kh,kw} \colon \mathbb{R}^{\nset{height}{h} \times \nset{width}{w}} &\rightarrow \mathbb{R}^{\nset{height}{h/kh} \times \nset{width}{w/kw}} \\
\text{maxpool2d}_{kh,hw}(X) &= \nfun{kh, kw}{max} U
\end{align*}
where
\begin{align*}
U &\in \reals^{\nset{height}{h / kh} \times \nset{width}{w / kw} \times \nset{kh}{kh} \times \nset{kw}{kw}} \\
U_{\nidx{height}{i}, \nidx{width}{j}, \nidx{kh}{di}, \nidx{kw}{dj}} & = X_{\nidx{height}{i \times kh + di -1}, \nidx{width}{j \times kw + dj -1}}.
\end{align*}

\subsubsection{Normalization layers}

Batch, instance, and layer normalization are often informally described using the same
equation, but they each correspond to very different functions. They differ
by which axes are normalized.

We can define a single generic normalization layer:
\begin{align*}
  \nfun{ax}{xnorm} \colon \mathbb{R}^{\name{ax}} &\rightarrow \mathbb{R}^{\name{ax}} \\
  \nfun{ax}{xnorm}(X; \gamma, \beta, \epsilon) &= \frac{X - \nfun{ax}{mean}(X)}{\sqrt{\nfun{ax}{var}(X)} + \epsilon} \odot \gamma + \beta
\end{align*}
where
\begin{align*}
  \gamma, \beta &\in \mathbb{R}^{\name{ax}} \\
  \epsilon &> 0.
\end{align*}

Now, suppose that the input has three axes:
\begin{align*}
X &\in \reals^{{\name{batch} \times \name{channels} \times \name{layer}}}
\end{align*}
Then the three kinds of normalization layers can be written as:
\begin{align*}
Y &= \nfun{batch}{xnorm}(X; \gamma, \beta) && \text{batch normalization} \\
Y &= \nfun{layer}{xnorm}(X; \gamma, \beta) && \text{instance normalization} \\
Y &= \nfun{layer,channels}{xnorm}(X; \gamma, \beta) && \text{layer normalization}
\end{align*}

\subsection{Transformer}
\label{sec:transformer}

We define a Transformer used autoregressively as a language model.
The input is a sequence of one-hot vectors, from which we compute word embeddings and positional encodings:
\begin{align*}
  I &\in \{0, 1\}^{\name{seq} \times \name{vocab}} & \nsum{vocab} I &= 1 \\
  W &= (E \ndot{vocab} I)\sqrt{|\name{layer}|} & E &\in \reals^{\name{vocab} \times \name{layer}} \\
  P &\in \reals^{\name{seq} \times \name{layer}} \\
  P_{\nidx{seq}{p}, \nidx{layer}{i}} &= \begin{cases}
    \sin((p-1) / 10000^{(i-1) / |\name{layer}|}) & \text{$i$ odd} \\ 
    \cos((p-1) / 10000^{(i-2) / |\name{layer}|}) & \text{$i$ even.}
  \end{cases}
\end{align*}

Then we use $L$ layers of self-attention and feed-forward neural networks:
\begin{align*} 
X^0 &= W+P \\
T^1 &= \text{LayerNorm}^1(\text{SelfAtt}^1(X^0)) + X^0\\
X^1 &= \text{LayerNorm}^{1'}(\text{FFN}^1(T^1)) + T^1\\
&\vdotswithin{=} \\
T^{L} &= \text{LayerNorm}^L(\text{SelfAtt}^L(X^{L-1})) + X^{L-1}\\
X^{L} &= \text{LayerNorm}^{L'}(\text{FFN}^L(T^L)) + T^L\\
O &= \nfun{vocab}{softmax}(E \ndot{layer} X^L)
\end{align*}
where $\text{LayerNorm}$, $\text{SelfAtt}$ and $\text{FFN}$ are defined below.

Layer normalization ($l = 1, 1', \ldots, L, L'$):
\begin{align*}
  \text{LayerNorm}^l \colon \mathbb{R}^{\name{layer}} &\rightarrow \mathbb{R}^{\name{layer}} \\
  \text{LayerNorm}^l(X) &= \nfun{layer}{xnorm}(X; \beta^l, \gamma^l).
\end{align*}

We defined attention in \S\ref{sec:attention}; the Transformer uses multi-head self-attention, in which queries, keys, and values are all computed from the same sequence.
\begin{align*}
  \text{SelfAtt}^l \colon \mathbb{R}^{\name{seq} \times \name{layer}} &\rightarrow \mathbb{R}^{\name{seq} \times \name{layer}} \\
  \text{SelfAtt}^l(X) &= Y
\end{align*}
where
\begin{align*}
  |\name{seq}| &= |\name{seq'}| \\
  |\name{key}| = |\name{val}| &= |\name{layer}|/|\name{heads}| \\
  Q &= \nmov{seq}{seq'}{W^{l,Q} \ndot{layer} X} & W^{l,Q} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{key}} \\
  K &= W^{l,K} \ndot{layer} X & W^{l,K} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{key}} \\
  V &= W^{l,V} \ndot{layer} X & W^{l,V} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{val}} \\
  M & \in \reals^{\name{seq} \times \name{seq'}} \\
  M_{\nidx{seq}{i}, \nidx{seq'}{j}} &= \begin{cases}
    0 & i \leq j\\
    -\infty & \text{otherwise}
  \end{cases} \\
  Y &= \nsum{heads} W^{l,O} \ndot{val} \nmov{seq'}{seq}{\text{Att}(Q, K, V, M)} & W^{l,O} &\in \mathbb{R}^{\name{heads} \times \name{val} \times \name{layer}}
\end{align*}

Feedforward neural networks:
\begin{align*}
  \text{FFN}^l \colon \mathbb{R}^{\name{layer}} &\rightarrow \mathbb{R}^{\name{layer}} \\
  \text{FFN}^l(X) &= X^2
\end{align*}
where
\begin{align*}
  X^1 &= \text{relu}(W^{l,1} \ndot{layer} X + b^{l,1}) & W^{l,1} &\in \mathbb{R}^{\name{hidden} \times \name{layer}} & b^{l,1} &\in \mathbb{R}^{\name{hidden}} \\
  X^2 &= \text{relu}(W^{l,2} \ndot{hidden} X^1 + b^{l,2}) & W^{l,2} &\in \mathbb{R}^{\name{layer} \times \name{hidden}} & b^{l,2} &\in \mathbb{R}^{\name{hidden}}.
\end{align*}

\subsection{LeNet}

\begin{align*}
X^0 &\in \reals^{\name{batch} \times \nset{channels}{c_0} \times \name{height} \times \name{width}} \\
T^1 &= \text{relu}(\text{conv}^1(X^0)) \\
X^1 &= \text{maxpool}^1(T^1) \\
T^2 &= \text{relu}(\text{conv}^2(X^1)) \\
X^2 &= \nmov{(height,width,channels)}{layer}{\text{maxpool}^2(T^2)} \\
X^3 &= \text{relu}(W^3 \ndot{layer} X^2 + b^3) & W^3 &\in \mathbb{R}^{\name{hidden} \times \name{layer}} & b^3 &\in \mathbb{R}^{\name{hidden}} \\
O &= \nfun{classes}{softmax} (W^4 \ndot{hidden} X^3 + b^4) & W^4 &\in \mathbb{R}^{\name{classes} \times \name{hidden}} & b^4 &\in \mathbb{R}^{\name{classes}}
\end{align*}
The flattening operation in the equation for $X^2$ is defined in \S{\ref{sec:reshaping}}. Alternatively, we could have written
\begin{align*}
X^2 &= \text{maxpool}^2(T^2) \\
X^3 &= \text{relu}(W^3 \ndot{height,width,channels} X^2 + b^3) & W^3 &\in \mathbb{R}^{\name{hidden} \times \name{height} \times \name{width} \times \name{channels}} & b^3 &\in \mathbb{R}^{\name{hidden}}
\end{align*}

The convolution and pooling operations are defined as follows:
\begin{align*}
\text{conv}^l(X) &= \nmov{channels'}{channels}{\text{conv2d}(X; W^l, b^l)}
\end{align*}
where
\begin{align*}
|\name{channels}| &= |\name{channels'}| \\
W^l & \in \reals^{\nset{channels'}{c_{l}} \times \nset{channels}{c_{l-1}} \times \nset{kh}{kh_l} \times \nset{kw}{kw_l}} \\
b^l &\in \reals^{\nset{channels'}{c_{l}}}
\end{align*}
and
\begin{align*}
\text{maxpool}^l(X) &= \text{maxpool2d}_{ph^l,ph^l}(X).
\end{align*}

\subsection{Other examples}

\subsubsection{Discrete random variables}

Named axes are very helpful for working with discrete random variables, because each random variable can be represented by an axis with the same name. For instance, if $\name{A}$ and $\name{B}$ are random variables, we can treat $p(\name{B} \mid \name{A})$ and $p(\name{A})$ as tensors:
\begin{align*}
p(\name{B} \mid \name{A}) &\in [0, 1]^{\name{A} \times \name{B}} & \nsum{B} p(\name{B}\mid \name{A}) &= 1 \\
(\name{A}) &\in [0, 1]^{\name{A}} & \nsum{A} p(\name{A}) &= 1
\end{align*}
Then Bayes' rule is just:
\begin{align*}
p(\name{A} \mid \name{B}) &= \frac{p(\name{B} \mid \name{A}) \odot p(\name{A})}{p(\name{B} \mid \name{A}) \ndot{A} p(\name{A})}.
\end{align*}

\subsubsection{Continuous bag of words}

A continuous bag-of-words model classifies by summing up the embeddings of a sequence of words $X$ and then projecting them to the space of classes. 

\begin{align*}
\text{cbow} \colon \{0, 1\}^{\name{seq} \times \name{vocab}} &\rightarrow \reals^{\name{seq} \times \name{classes}} \\
\text{cbow}(X; E, W) &= \nfun{class}{softmax} (W \ndot{hidden} E \ndot{vocab} X)
\end{align*}
where
\begin{align*}
\nsum{vocab} X &= 1 \\
E &\in \reals^{\name{vocab} \times \name{hidden}} \\
W &\in \reals^{\name{classes} \times \name{hidden}}.
\end{align*}
Here, the two contractions can be done in either order, so we leave the parentheses off.

\subsubsection{Sudoku ILP}

Sudoku puzzles can be represented as  binary tiled tensors.
Given a grid we can check that it is valid by converting it to a grid of grids. 
Constraints then ensure that there is one digit per row, per column and per sub-box.

\begin{align*}
\text{check} \colon \{0, 1\}^{\nset{height}{9} \times \nset{width}{9} \times \nset{assign}{9}} &\rightarrow \{0, 1\} \\
\text{check}(X) &=
\mathbb{I}\left[\begin{aligned}
\nsum{assign} Y = 1 &\land \nsum{height, width} Y = 1 \land {} \\
\nsum{Height, height} Y = 1 &\land \nsum{Width, width} Y = 1
\end{aligned}\right]
\end{align*}
where
\begin{align*}
Y &\in \{0, 1\}^{\nset{Height}{3} \times \nset{Width}{3} \times \nset{height}{3} \times \nset{width}{3} \times \nset{assign}{9}}  \\
Y_{\nidx{Height}{r}, \nidx{height}{r'}, \nidx{Width}{c}, \nidx{width}{c'}} &= X_{\nidx{height}{r\times 3 + r'-1}, \nidx{width}{c\times3 + c'-1}}.
\end{align*} 

\subsubsection{$K$-means clustering}

The following equations define one step of $k$-means clustering. Given a set of points $X$ and an initial set of cluster centers $C$,
\begin{align*}
  X &\in \reals^{\name{batch} \times \name{space}} \\
C &\in \reals^{\name{clusters} \times \name{space}}
\end{align*}
we repeat the following update: Compute cluster assignments
\begin{align*}
Q &= \nfun{clusters}{argmin} \nfun{space}{norm}(C-X)
\end{align*}
then recompute the cluster centers:
\begin{equation*}
C \leftarrow \nsum{batch} \frac{Q \odot X}{Q}.
\end{equation*}

\subsubsection{Beam search}

Beam search is a commonly used approach for approximate discrete search. Here $H$ is the score of each element in the beam, $S$ is the state of each element in the beam, and $f$ is an update function that returns the score of each state transition. 
\begin{align*} 
H &\in \reals^{\name{beam}} \\
S &\in \{0, 1\}^{\name{beam} \times \name{state}} & \nsum{state} S &= 1 \\
f &\colon \{0, 1\}^{\name{state}} \rightarrow \reals^{\name{state}} \\
\end{align*}
Then we repeat the following update:
\begin{align*}
H' &= \nfun{beam}{max} (H \odot f(S)) \\
H &\leftarrow \nfun{state,beam}{maxk} H' \\
S &\leftarrow \nfun{state,beam}{argmaxk} H'
\end{align*}
where
\begin{align*}
\nfun{ax,k}{maxk} \colon \reals^{\name{ax}} &\rightarrow \reals^{\name{k}} \\
\nfun{ax,k}{argmaxk} \colon \reals^{\name{ax}} &\rightarrow \{0,1\}^{\name{ax},\name{k}}
\end{align*}
are defined such that $[\nfun{ax,k}{maxk} A]_{\nidx{k}{i}}$ is the $i$-th largest value along axis $\name{ax}$ and $A \ndot{ax} (\nfun{ax,k}{argmaxk}{A}) = \nfun{ax,k}{max} A$.

We can add a \name{batch} axis to $H$ and $S$ and the above equations will work unchanged.

\subsubsection{Multivariate normal distribution}

In our notation, the application of a bilinear form is more verbose than the standard notation ($(X-\mu)^\top \Sigma^{-1} (X-\mu)$), but also makes it look more like a function of two arguments (and would generalize to three or more arguments).

\begin{align*}
\mathcal{N} \colon \reals^{\name{d}} &\rightarrow \reals \\
\mathcal{N}(X; \mu, \Sigma) &= \frac{\displaystyle \exp\left(-\frac{1}{2}  \left(\nfun{d1, d2}{inv}(\Sigma) \ndot{d1} \nmov{d}{d1}{X - \mu} \right) \ndot{d2} \nmov{d}{d2}{X - \mu} \right)}{\sqrt{(2 \pi)^{|\name{d}|} \nfun{d1, d2}{det}(\Sigma)}}
\end{align*}
where
\begin{align*}
|\name{d}| &= |\name{d1}| = |\name{d2}| \\
\mu &\in \reals^{\name{d}} \\
\Sigma & \in \reals^{\name{d1} \times \name{d2}}.
\end{align*}

\section{\LaTeX{} Macros}

Many of the \LaTeX{} macros used in this document are available in the style file \url{https://namedtensor.github.io/namedtensor.sty}. To use it, put
\begin{quote}
\begin{verbatim}
\usepackage{namedtensor}
\end{verbatim}
\end{quote}
in the preamble of your \LaTeX{} source file (after \verb|\documentclass{article}| but before \verb|\begin{document}|).

The style file contains a small number of macros:
\begin{itemize}
\item Use \verb|\name{foo}| to write an axis name: $\name{foo}$.
\item Use \verb|A \ndot{foo} B| for contraction: $A \ndot{foo} B$. Similarly, use \verb|A \ncat{foo} B| for concatenation.
\item Use \verb|\nsum{foo} A| for summation: $\nsum{foo} A$.
\item Use \verb|\nfun{foo}{qux} A| for a function named qux with a name under it: $\nfun{foo}{qux} A$.
\item Use \verb|\nmov{foo}{bar}{A}| for renaming: $\nmov{foo}{bar}{A}$.
\end{itemize}

\section{Formal Definitions}
\label{sec:definitions}

% ugly hack to get non-sans-serif into names
\newcommand{\sub}[1]{_\text{$#1$}}

\subsection{Axes, shapes, records, and types}

A \emph{named index} is a pair $(\text{ax},i)$, written $\nidx{ax}{i}$, where $\name{ax}$ is a \emph{name} and $i$ is usually a natural number.
%We write both names and variables ranging over names using sans-serif font.
An \emph{axis} is a set of named indices $\{ (\text{ax}, i) | i \in I \}$ all sharing the same name. 
We deal with axes of the form $\nset{ax}{[n]}$ (that is, $\nset{ax}{\{1, \ldots, n\}}$) so frequently that we abbreviate this as~$\nset{ax}{n}$.
We sometimes use $\name{ax}$ to specify the abstract axis, without specifying the set of its support, when the size can be inferred from context or when the particular size is immaterial (for example when defining a polymorphic function that can take tensors with given axes regardless of their size).

A \emph{type} is a set of (distinct) axes, without specifying their support size. A \emph{shape} is a set of axes with the support specified.
For example, $\{ \foo ,\bar \}$ is a type and $\{ \foo[10] , \bar[20] \}$ is a shape.
If $\mathcal{S} = \{ \ax^1,\ldots, \ax^d \}$ is a shape then a \emph{record} of shape $\mathcal{S}$ is a set $\{ i_1,\ldots, i_d \}$ where $i_j \in \ax^j$.
We denote by $\rec \mathcal{S}$ the set of all records of shape $\mathcal{S}$.
A \emph{tensor} of shape $\mathcal{S}$ is a function mapping all records of shape $\mathcal{S}$ into the real numbers.
We write $A \in \R^{\ax^1 \times \cdots \times \ax^d}$ for the set of all tensors of the shape $\{ \ax^1,\ldots,\ax^d \}$.
If $A$ is a tensor of shape $\mathcal{S}$ and $r \in \rec \mathcal{S}$ then we denote by $A_r$ the real numbered map into by $r$.




A \emph{partial record} of shape $\mathcal{S}= \{ \ax^1 ,\ldots, \ax^d \}$ is a set $\{ \i_1,\ldots, i_{d'} \}$  such that $d' \leq d$ and each of the $i_j$'s is a member of a distinct element of $\mathcal{S}$. If $r = \{ i_1 ,\ldots, i_{d'} \}$ is a partial record of shape $\mathcal{S}$, then we denote by $\mathcal{S}(r)$ the set $\{ \ax^1 , \ldots \ax^{d'} \} \subseteq \mathcal{S}$ such that $i_j \in \ax^j$ for $j = 1,\ldots, d'$.
If $A$ is a tensor of shape $\mathcal{S} = \{ \ax^1,\ldots, \ax^d \}$ and $r = \{ i_1,\ldots, i_{d'} \}$ is a partial record of shape $\mathcal{S}$, then $A_r$ is the tensor of shape $\mathcal{S} \setminus \mathcal{S}(r)$ that maps every record $r' = \{ j_1,\ldots, j_{d-d'} \}$ to the value  $A_{r \cup r'}$.


%If $t \in \rec \mathcal{T}$ and $\mathcal{S} \subseteq \mathcal{T}$, then we write $\restrict{t}{\mathcal{S}}$ for the unique record in $\rec \mathcal{S}$ such that $\restrict{t}{\mathcal{S}} \subseteq t$.


\iffalse 

\subsection{Named tensors}

Let $F$ be a field and let $\mathcal{S}$ be a shape. Then a \emph{named tensor over $F$ with shape $\mathcal{S}$} is a mapping from $\mathcal{S}$ to $F$. We write the set of all named tensors with shape $\mathcal{S}$ as $F^{\mathcal{S}}$.

We don't make any distinction between a scalar (an element of $F$) and a named tensor with empty shape (an element of $F^\emptyset$).

If $A \in F^{\mathcal{S}}$, then we access an element of $A$ by applying it to a record $s \in \rec \mathcal{S}$; but we write this using the usual subscript notation: $A_s$ rather than $A(s)$. To avoid clutter, in place of $A_{\{\nidx{ax\sub{1}}{x_1}, \ldots, \nidx{ax\sub{r}}{x_r}}\}$, we usually write $A_{\nidx{ax\sub{1}}{x_1}, \ldots, \nidx{ax\sub{r}}{x_r}}$. When a named tensor is an expression like $(A+B)$, we surround it with square brackets like this: $[A+B]_{\nidx{ax\sub{1}}{x_1}, \ldots, \nidx{ax\sub{r}}{x_r}}$.

We also allow partial indexing. If $A$ is a tensor with shape $\mathcal{T}$ and $s \in \rec \mathcal{S}$ where $\mathcal{S} \subseteq \mathcal{T}$, then we define $A_s$ to be the named tensor with shape $\mathcal{T} \setminus \mathcal{S}$ such that, for any $t \in \rec (\mathcal{T} \setminus \mathcal{S})$,
\begin{align*}
\left[A_s\right]_t &= A_{s \cup t}.
\end{align*}
(For the edge case $\mathcal{T} = \emptyset$, our definitions for indexing and partial indexing coincide: one gives a scalar and the other gives a tensor with empty shape, but we don't distinguish between the two.)

\fi 

\subsection{Extending functions to named tensors}
\label{sec:tensorfunctions}

In \S\ref{sec:intro}, we described several classes of functions that can be extended to named tensors. Here, we define how to do this for general functions.

We say two shapes $\mathcal{S}$ and $\mathcal{T}$ are \emph{compatible} if whenever $\nidx{ax}{I} \in \mathcal{S}$ and $\nidx{ax}{J} \in \mathcal{T}$, then $I = J$. We say that $\mathcal{S}$ and $\mathcal{T}$ are \emph{orthogonal} if there is no $\name{ax}$ such that $\nidx{ax}{I} \in \mathcal{S}$ and $\nidx{ax}{J} \in \mathcal{T}$ for any $I$, $J$.


Let $f \colon F^{\mathcal{S}} \rightarrow G^{\mathcal{T}}$ be a function from tensors to tensors. For any shape $\mathcal{U}$ orthogonal to both $\mathcal{S}$ and $\mathcal{T}$, we can extend $f$ to:
\begin{align*}
f &: F^{\mathcal{S} \cup \mathcal{U}} \rightarrow G^{\mathcal{T} \cup \mathcal{U}} \\
[f(A)]_u &= f(A_u) \qquad \text{for all $u \in \rec\mathcal{U}$.}
\end{align*}

If $f$ is a multary function, we can extend its arguments to larger shapes, and we don't have to extend all the arguments with the same names. We consider just the case of two arguments; three or more arguments are analogous. Let $f \colon F^{\mathcal{S}} \times G^{\mathcal{T}} \rightarrow H^{\mathcal{U}}$ be a binary function from tensors to tensors. For any shapes $\mathcal{S'}$ and $\mathcal{T'}$ that are compatible with each other and orthogonal to $\mathcal{S}$ and $\mathcal{T}$, respectively, and $\mathcal{U'} = \mathcal{S'} \cup \mathcal{T'}$ is orthogonal to $\mathcal{U}$, we can extend $f$ to:
\begin{align*}
f &: F^{\mathcal{S} \cup \mathcal{S'}} \times G^{\mathcal{T} \cup \mathcal{T'}} \rightarrow H^{\mathcal{U} \cup \mathcal{U'}} \\
  [f(A,B)]_u &= f\left(A_{\restrict{u}{\mathcal{S'}}},B_{\restrict{u}{\mathcal{T'}}}\right) \qquad \text{for all $u \in \rec\mathcal{U'}$.}
\end{align*}

All of the tensor operations described in \S\ref{sec:operations} can be defined in this way. For example, the contraction operator extends the following ``named dot-product'':
\begin{align*}
\ndot{ax} &: F^{\nset{ax}{n}} \times F^{\nset{ax}{n}} \rightarrow F \\
A \ndot{ax} B &= \sum_{i=1}^n A_{\nidx{ax}{i}} B_{\nidx{ax}{i}}.
\end{align*}

\section{Extensions}

\input{types}
\input{dual}
\input{fancy}

\section*{Acknowledgements}

Thanks to Ekin Aky\"{u}rek, Colin McDonald, Chung-chieh Shan, and Nishant Sinha for their input to this document (or the ideas in it).

\iffalse % hack to make this heading appear only in pandoc
\section*{References}
\fi

\bibliographystyle{acl_natbib}
\bibliography{references}

\end{document}
